#!/usr/bin/env python3
"""
æ¯æ—¥èµ›é¢˜æ–‡ä»¶ä¸‹è½½è„šæœ¬
ä» FormaLLM ç«èµ› API è·å–æ¯æ—¥èµ›é¢˜å¹¶ä¿å­˜åˆ°æ–‡ä»¶
"""

import requests
import json
import os
import sys
from datetime import datetime, timezone, timedelta
import pytz

# API é…ç½®
API_BASE_URL = os.getenv("FORMALLM_API_BASE", "http://121.43.230.124")
API_KEY = os.getenv("FORMALLM_API_KEY", "default_api_key")

# æ–‡ä»¶ä¿å­˜ç›®å½•
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)
FILES_DIR = os.path.join(PROJECT_ROOT, "assets", "files")


def fetch_daily_problems(date=None, track="all"):
    """
    è·å–æ¯æ—¥èµ›é¢˜
    
    Args:
        date: æ—¥æœŸ (YYYY-MM-DD æ ¼å¼)ï¼Œé»˜è®¤ä¸ºä»Šå¤©
        track: èµ›é“ ("lean", "litex", æˆ– "all")
    
    Returns:
        dict: {
            "date": "2025-11-06",
            "lean": [...],
            "litex": [...]
        }
    """
    if date is None:
        # ä½¿ç”¨åŒ—äº¬æ—¶é—´ï¼ˆUTC+8ï¼‰
        tz_beijing = timezone(timedelta(hours=8))
        date = datetime.now(tz_beijing).strftime('%Y-%m-%d')
    
    # è¿™é‡Œéœ€è¦æ ¹æ®æ‚¨çš„å®é™… API ç«¯ç‚¹è°ƒæ•´
    # å‡è®¾ API æ ¼å¼ä¸º: /problems/daily?date=YYYY-MM-DD&track=lean
    
    result = {
        "date": date,
        "lean": [],
        "litex": []
    }
    
    if track in ["all", "lean"]:
        url = f"{API_BASE_URL}/problems/daily"
        headers = {"X-API-Key": API_KEY}
        params = {"date": date, "track": "lean"}
        
        print(f"ğŸ“¡ è·å– Lean èµ›é¢˜ (æ—¥æœŸ: {date})...")
        try:
            response = requests.get(url, headers=headers, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            if "error" not in data and isinstance(data, list):
                result["lean"] = data
                print(f"âœ… æˆåŠŸè·å– Lean èµ›é¢˜: {len(data)} é¢˜")
            else:
                print(f"âš ï¸  Lean èµ›é¢˜æš‚æ— æ•°æ®")
        except requests.exceptions.RequestException as e:
            print(f"âŒ Lean èµ›é¢˜è·å–å¤±è´¥: {e}")
    
    if track in ["all", "litex"]:
        url = f"{API_BASE_URL}/problems/daily"
        headers = {"X-API-Key": API_KEY}
        params = {"date": date, "track": "litex"}
        
        print(f"ğŸ“¡ è·å– Litex èµ›é¢˜ (æ—¥æœŸ: {date})...")
        try:
            response = requests.get(url, headers=headers, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            if "error" not in data and isinstance(data, list):
                result["litex"] = data
                print(f"âœ… æˆåŠŸè·å– Litex èµ›é¢˜: {len(data)} é¢˜")
            else:
                print(f"âš ï¸  Litex èµ›é¢˜æš‚æ— æ•°æ®")
        except requests.exceptions.RequestException as e:
            print(f"âŒ Litex èµ›é¢˜è·å–å¤±è´¥: {e}")
    
    return result


def save_problems_to_files(problems_data):
    """
    å°†èµ›é¢˜æ•°æ®ä¿å­˜ä¸º JSONL æ–‡ä»¶ï¼ˆç›´æ¥å¤åˆ¶å·²æœ‰æ–‡ä»¶ï¼‰
    
    Args:
        problems_data: åŒ…å«æ—¥æœŸå’Œèµ›é¢˜çš„å­—å…¸ï¼Œæ”¯æŒä»é¢„ç½®æ–‡ä»¶å¤åˆ¶
    
    Returns:
        list: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    date = problems_data["date"]
    date_str = date.replace("-", "")  # 20251106
    saved_files = []
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(FILES_DIR, exist_ok=True)
    
    # ä¿å­˜ Lean èµ›é¢˜
    if problems_data.get("lean_file"):
        # å¦‚æœæä¾›äº†æºæ–‡ä»¶è·¯å¾„ï¼Œç›´æ¥å¤åˆ¶
        source_file = problems_data["lean_file"]
        filename = f"lean_{date_str[4:]}.jsonl"  # lean_1106.jsonl
        filepath = os.path.join(FILES_DIR, filename)
        
        import shutil
        shutil.copy2(source_file, filepath)
        print(f"ğŸ’¾ Lean èµ›é¢˜å·²å¤åˆ¶: {source_file} -> {filename}")
        saved_files.append(filepath)
    elif problems_data["lean"]:
        # API æ–¹å¼ä¿å­˜
        filename = f"lean_{date_str[4:]}.jsonl"
        filepath = os.path.join(FILES_DIR, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            for problem in problems_data["lean"]:
                f.write(json.dumps(problem, ensure_ascii=False) + '\n')
        
        print(f"ğŸ’¾ Lean èµ›é¢˜å·²ä¿å­˜: {filename} ({len(problems_data['lean'])} é¢˜)")
        saved_files.append(filepath)
    
    # ä¿å­˜ Litex èµ›é¢˜
    if problems_data.get("litex_file"):
        # å¦‚æœæä¾›äº†æºæ–‡ä»¶è·¯å¾„ï¼Œç›´æ¥å¤åˆ¶
        source_file = problems_data["litex_file"]
        filename = f"litex_{date_str[4:]}.jsonl"
        filepath = os.path.join(FILES_DIR, filename)
        
        import shutil
        shutil.copy2(source_file, filepath)
        print(f"ğŸ’¾ Litex èµ›é¢˜å·²å¤åˆ¶: {source_file} -> {filename}")
        saved_files.append(filepath)
    elif problems_data["litex"]:
        # API æ–¹å¼ä¿å­˜
        filename = f"litex_{date_str[4:]}.jsonl"
        filepath = os.path.join(FILES_DIR, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            for problem in problems_data["litex"]:
                f.write(json.dumps(problem, ensure_ascii=False) + '\n')
        
        print(f"ğŸ’¾ Litex èµ›é¢˜å·²ä¿å­˜: {filename} ({len(problems_data['litex'])} é¢˜)")
        saved_files.append(filepath)
    
    return saved_files


def update_downloads_json(problems_data, saved_files):
    """
    æ›´æ–° downloads.json é…ç½®æ–‡ä»¶
    
    Args:
        problems_data: èµ›é¢˜æ•°æ®
        saved_files: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    downloads_json_path = os.path.join(PROJECT_ROOT, "assets", "data", "downloads.json")
    
    if not os.path.exists(downloads_json_path):
        print("âš ï¸  downloads.json ä¸å­˜åœ¨ï¼Œè·³è¿‡æ›´æ–°")
        return
    
    # è¯»å–ç°æœ‰é…ç½®
    with open(downloads_json_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    # å‡†å¤‡æ–°çš„èµ›é¢˜æ¡ç›®
    date = problems_data["date"]
    date_obj = datetime.strptime(date, '%Y-%m-%d')
    
    # å¦‚æœå½“å‰æ—¶é—´åœ¨åŒ—äº¬æ—¶é—´23:00ä¹‹åï¼Œtimestamp å†™å…¥æ˜å¤©çš„æ—¥æœŸ
    # è¿™æ ·å‰ç«¯åœ¨23:00åå°±èƒ½åŒ¹é…åˆ°"æ˜å¤©"çš„èµ›é¢˜
    now_bj = datetime.now(pytz.timezone('Asia/Shanghai'))
    if now_bj.hour >= 23:
        # æ—¶é—´æˆ³ä½¿ç”¨æ˜å¤©çš„æ—¥æœŸ
        display_date = date_obj + timedelta(days=1)
        timestamp = display_date.strftime('%Y-%m-%d %H:%M:%S')
    else:
        timestamp = date_obj.strftime('%Y-%m-%d %H:%M:%S')
    
    items = []
    for filepath in saved_files:
        filename = os.path.basename(filepath)
        
        # è®¡ç®— MD5
        import hashlib
        md5_hash = hashlib.md5()
        with open(filepath, 'rb') as f:
            md5_hash.update(f.read())
        md5 = md5_hash.hexdigest()
        
        # ç¡®å®šèµ›é“åç§°
        if filename.startswith("lean"):
            name = f"Lean èµ›é¢˜ ({date_obj.strftime('%mæœˆ%dæ—¥')})"
        elif filename.startswith("litex"):
            name = f"Litex èµ›é¢˜ ({date_obj.strftime('%mæœˆ%dæ—¥')})"
        else:
            name = filename
        
        items.append({
            "name": name,
            "md5": md5,
            "url": "https://www.xir.cn/competitions/1143",
            "local": f"assets/files/{filename}",
            "available": True
        })
    
    # æ„å»ºæ–°çš„æ•°æ®é›†æ¡ç›®
    new_dataset = {
        "timestamp": timestamp,
        "title": f"{date_obj.month}æœˆ{date_obj.day}æ—¥èµ›é¢˜",
        "note": "æŠ¥ååå¯ä¸‹è½½æ•°æ®",
        "items": items
    }
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒæ—¥æœŸçš„èµ›é¢˜
    existing_index = None
    for i, dataset in enumerate(config["datasets"]):
        if dataset.get("timestamp", "").startswith(date):
            existing_index = i
            break
    
    # æ›´æ–°æˆ–æ’å…¥æ–°æ¡ç›®
    if existing_index is not None:
        config["datasets"][existing_index] = new_dataset
        print(f"ğŸ”„ æ›´æ–°å·²å­˜åœ¨çš„èµ›é¢˜æ¡ç›®: {date}")
    else:
        config["datasets"].insert(0, new_dataset)
        print(f"â• æ·»åŠ æ–°çš„èµ›é¢˜æ¡ç›®: {date}")
    
    # æ›´æ–° lastUpdated
    tz_beijing = timezone(timedelta(hours=8))
    config["lastUpdated"] = datetime.now(tz_beijing).isoformat()
    
    # ä¿å­˜é…ç½®
    with open(downloads_json_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… downloads.json å·²æ›´æ–°")
    
    return config  # è¿”å›æ›´æ–°åçš„é…ç½®ï¼Œç”¨äºæ›´æ–° HTML


def update_html_embedded_json(config):
    """
    æ›´æ–° HTML æ–‡ä»¶ä¸­çš„å†…åµŒ JSON æ•°æ®
    
    Args:
        config: æ›´æ–°åçš„ downloads.json é…ç½®æ•°æ®
    """
    import re
    
    html_files = [
        os.path.join(PROJECT_ROOT, "cn", "downloads.html"),
        os.path.join(PROJECT_ROOT, "en", "downloads.html")
    ]
    
    # å‡†å¤‡ JSON å­—ç¬¦ä¸²ï¼ˆæ ¼å¼åŒ–ï¼Œ2 ç©ºæ ¼ç¼©è¿›ï¼‰
    json_str = json.dumps(config, ensure_ascii=False, indent=2)
    
    for html_path in html_files:
        if not os.path.exists(html_path):
            print(f"âš ï¸  HTML æ–‡ä»¶ä¸å­˜åœ¨: {html_path}")
            continue
        
        try:
            # è¯»å– HTML æ–‡ä»¶
            with open(html_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… <script id="downloads-data"> æ ‡ç­¾å†…çš„ JSON
            # åŒ¹é…æ¨¡å¼ï¼šä» <script id="downloads-data" å¼€å§‹ï¼Œåˆ° </script> ç»“æŸï¼ˆæ”¯æŒå¤šè¡Œï¼‰
            pattern = r'(<script\s+id=["\']downloads-data["\'][^>]*>)\s*(\{[\s\S]*?\})\s*(</script>)'
            
            def replace_json(match):
                script_open = match.group(1)
                script_close = match.group(3)
                # è¿”å›æ–°çš„ script æ ‡ç­¾ï¼ŒåŒ…å«æ›´æ–°åçš„ JSONï¼ˆä¿æŒåŸæœ‰ç¼©è¿›ï¼‰
                # æŸ¥æ‰¾ script æ ‡ç­¾å‰çš„ç¼©è¿›
                match_start = match.start()
                lines = html_content[:match_start].split('\n')
                if lines:
                    indent = len(lines[-1]) - len(lines[-1].lstrip())
                    indent_str = ' ' * indent
                else:
                    indent_str = '          '  # é»˜è®¤ 10 ä¸ªç©ºæ ¼
                return f"{script_open}\n{json_str}\n{indent_str}{script_close}"
            
            # æ›¿æ¢ JSON å†…å®¹
            new_html = re.sub(pattern, replace_json, html_content, flags=re.DOTALL)
            
            # å¦‚æœæ›¿æ¢æˆåŠŸï¼ˆå†…å®¹æœ‰å˜åŒ–ï¼‰
            if new_html != html_content:
                # å†™å›æ–‡ä»¶
                with open(html_path, 'w', encoding='utf-8') as f:
                    f.write(new_html)
                print(f"âœ… HTML å†…åµŒæ•°æ®å·²æ›´æ–°: {os.path.basename(html_path)}")
            else:
                print(f"âš ï¸  HTML æ–‡ä»¶æœªæ‰¾åˆ°åŒ¹é…çš„ JSON å—: {os.path.basename(html_path)}")
                
        except Exception as e:
            print(f"âŒ æ›´æ–° HTML æ–‡ä»¶å¤±è´¥ {os.path.basename(html_path)}: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ“ FormaLLM æ¯æ—¥èµ›é¢˜ä¸‹è½½")
    print("=" * 60)
    print()
    
    # è·å–å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        global API_KEY
        API_KEY = sys.argv[1]
        print(f"ğŸ”‘ ä½¿ç”¨è‡ªå®šä¹‰ API Key")
    
    date = sys.argv[2] if len(sys.argv) > 2 else None
    track = sys.argv[3] if len(sys.argv) > 3 else "all"
    
    print()
    
    # 1. è·å–èµ›é¢˜æ•°æ®
    problems_data = fetch_daily_problems(date, track)
    
    if not problems_data["lean"] and not problems_data["litex"]:
        print()
        print("âš ï¸  ä»Šæ—¥æš‚æ— èµ›é¢˜æ•°æ®")
        return
    
    # 2. ä¿å­˜ä¸º JSONL æ–‡ä»¶
    print()
    saved_files = save_problems_to_files(problems_data)
    
    if not saved_files:
        print("âš ï¸  æ²¡æœ‰ä¿å­˜ä»»ä½•æ–‡ä»¶")
        return
    
    # 3. æ›´æ–° downloads.json
    print()
    config = update_downloads_json(problems_data, saved_files)
    
    # 4. æ›´æ–° HTML æ–‡ä»¶ä¸­çš„å†…åµŒ JSON
    if config:
        print()
        update_html_embedded_json(config)
    
    print()
    print("=" * 60)
    print("âœ… èµ›é¢˜ä¸‹è½½å®Œæˆï¼")
    print(f"ğŸ“ ä¿å­˜æ–‡ä»¶æ•°: {len(saved_files)}")
    print("=" * 60)


if __name__ == "__main__":
    main()

